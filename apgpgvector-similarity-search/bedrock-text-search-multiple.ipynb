{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01cb58e",
   "metadata": {},
   "source": [
    "# Building AI-powered image search in PostgreSQL using Amazon Bedrock and pgvector\n",
    "_**Using a pretrained LLM and PostgreSQL extension `pgvector` for similarity image search on product catalog**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Amazon Bedrock Model Hosting](#Amazon-Bedrock-Model-Hosting)\n",
    "1. [Load data into PostgreSQL](#Open-source-extension-pgvector-in-PostgreSQL)\n",
    "1. [Evaluate Search Results](#Evaluate-PostgreSQL-vector-Search-Results)\n",
    "\n",
    "## Background\n",
    "\n",
    "Image search refers to the process of using an image as a query to find related or similar images. Users provide an image as a query, and the search engine uses visual features such as color, shape, and texture to find visually similar images. This type of search is more advanced and relies on image recognition and computer vision technologies. Image search is useful for various purposes, such as finding the source of an image, identifying objects or landmarks in a picture, or discovering visually similar content\n",
    "\n",
    "In this notebook, we'll build the core components of a visually similar Products. Often people don't know what exactly they are looking for and in that case they just type an item description or upload a photo of a product and looking for similar products matching those items.\n",
    "\n",
    "One of the core components of searching visually similar items is a fixed length sentence/word embedding i.e. a  “feature vector” that corresponds to that image. For image search, we will convert the product images into \"feature vector\" that corresponds to that image. The reference image embedding typically are generated offline and must be stored so they can be efficiently searched. In this use case we are using a pretrained multimodal `amazon.titan-embed-image-v1` embeddings from [Amazon Titan](https://aws.amazon.com/bedrock/titan/). \n",
    "\n",
    "To enable efficient searches for visually similar items, we'll use  `amazon.titan-embed-image-v1` to generate fixed length sentence embeddings i.e “feature vectors” and use the Nearest Neighbor search in Amazon Aurora for PostgreSQL using the extension `pgvector`. The PostgreSQL `pgvector` extension lets you store and search for points in vector space and find the \"nearest neighbors\" for those points. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items:\n",
    "\n",
    "- Generate feature vectors for the products images from [Kaggle dataset]((https://www.kaggle.com/datasets/vikashrajluhaniwal/fashion-images/) using Amazon Titan Multimodal Embeddings.\n",
    "- Store the generated vectors in Amazon Aurora PostgreSQL with the pgvector extension along with the metadata\n",
    "- Explore some sample text queries, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7045906",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required python libraries for the workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pgvector boto3 pandarallel psycopg ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3cadd",
   "metadata": {},
   "source": [
    "### Downloading Fashion image dataset from Kaggle\n",
    "\n",
    "The dataset itself consists of 2900+  product images under Apparel and Footwear category. Two gender types Boys and Girls under Apparel, similarly Men and Women under Footwear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256ee1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Loading configuration information\n",
    "fp = open('data/config.json')\n",
    "data = json.load(fp)\n",
    "\n",
    "# Generating list of embedding models available \n",
    "bedrock = boto3.client(service_name='bedrock', region_name='us-west-2')\n",
    "listModels = bedrock.list_foundation_models()\n",
    "\n",
    "model_list = []\n",
    "for model in listModels['modelSummaries']:\n",
    "    if 'EMBEDDING' in model['outputModalities'] and 'ON_DEMAND' in model['inferenceTypesSupported']:\n",
    "        model_list.append(model['modelId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c4b9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Select Dataset for this lab"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ac0b0d9405483087c352ce58353b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', layout=Layout(width='max-content'), options={'Fashion Dataset': 'dress.csv', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Select an Embedding Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70952b71790b4710937d489584c119c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embeddings Models: ', layout=Layout(width='max-content'), options=('amazon.titan-embed-g…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this section we will choose the dataset and the embedding model for this lab\n",
    "\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "\n",
    "dataset = widgets.Dropdown(\n",
    "    options=data['dataset'],\n",
    "    description=\"Dataset:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "\n",
    "model = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    description=\"Embeddings Models: \",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select Dataset for this lab\"))\n",
    "display(dataset)\n",
    "\n",
    "display(IPython.display.Markdown(\"## Select an Embedding Model\"))\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef202ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data of csv\n",
    "df = pd.read_csv('data/{}'.format(dataset.value))\n",
    "df = df[['descriptions','display','image_url']]\n",
    "print(\"Total number of records : {}\".format(len(df.index)))\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa2f4b",
   "metadata": {},
   "source": [
    "# Amzon Bedrock Model Hosting\n",
    "\n",
    "In this section will deploy the pretrained `amazon.titan-embed-image-v1` model into Amazon Bedrock and generate 1024 dimensional vector embeddings for our product catalog images.\n",
    "\n",
    "**Note**\n",
    "Please make sure you have completed Amazon Bedrock Setup before continuing this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4718c2",
   "metadata": {},
   "source": [
    "Function to convert the image into vector embeddings. This function will be called for all the individual product images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3480d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(query):\n",
    "    \n",
    "    if model.value.startswith('amazon'):\n",
    "        payLoad = json.dumps({'inputText': query })\n",
    "    else:\n",
    "        payLoad = json.dumps({'texts': [query],'input_type': 'search_document'})\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=payLoad, \n",
    "        modelId=model.value, \n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\" )\n",
    "       \n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    if model.value.startswith('amazon'):\n",
    "        output = response_body.get(\"embedding\")\n",
    "    else:\n",
    "        output = response_body.get(\"embeddings\")[0]\n",
    "    return output\n",
    "\n",
    "description_embeddings = generate_embeddings(df.iloc[1].get('descriptions'))\n",
    "no_of_embeddings = len(description_embeddings)\n",
    "\n",
    "print(\"Embedding model : {}\".format(model.value))\n",
    "print (\"Number of dimensions : {}\".format(len(description_embeddings)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfea36",
   "metadata": {},
   "source": [
    "In this code block, we will scan through all the data in the dataframe for the image stored in the ImageURL column and convert it as embeddings using Amazon Tital Multimodal Embeddings and store it as image_embeddings column in the same dataframe. We will use [pandarallel](https://pypi.org/project/pandarallel/) to parallize the generation of vector embeddings. pandarallel is a simple and efficient tool to parallelize Pandas operations on all available CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7073d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all the products descriptions - approx 3 min to complete\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
    "\n",
    "df['embeddings'] = df['descriptions'].parallel_apply(generate_embeddings)\n",
    "df.head()\n",
    "\n",
    "print(\"Completed generation of embeddings for all the products images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e107d",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector in PostgreSQL\n",
    "\n",
    "pgvector is an open-source extension for PostgreSQL offers a powerful and versatile way to store, manipulate, and search for vector data within PostgreSQL. Its features and ease of use make it a valuable tool for various applications like recommendation systems, image and text retrieval, and data clustering.\n",
    "\n",
    "Here are some of the key features of the pgvector extension for PostgreSQL:\n",
    "\n",
    "**Vector Storage and Operations:**\n",
    "\n",
    "- **Dedicated vector type**: Stores vectors directly in tables, providing efficient storage and retrieval.   \n",
    "- **Multiple data types:** Supports various data types for vector elements, including floats, integers, and strings.   \n",
    "- **Basic vector operations:** Allows basic mathematical operations like addition, subtraction, scaling, and dot product.   \n",
    "\n",
    "**Similarity Search:**\n",
    "\n",
    "- **Exact and approximate nearest neighbor search:** Find the closest data points based on similarity metrics like L2 distance, inner product, and cosine distance.    \n",
    "- **Trade-off between accuracy and performance:** Exact search offers perfect recall but slower speed, while approximate search sacrifices some accuracy for higher speed.    \n",
    "- **Multiple indexing options:** Supports HNSW and IVFFlat indexes for optimizing similarity search queries.   \n",
    "\n",
    "**Integration and Ease of Use:**    \n",
    "\n",
    "- **Seamless SQL integration:** Use pgvector functions and operators directly within your SQL queries for a familiar workflow.   \n",
    "- **Multiple programming language support:** Works with any client library that connects to PostgreSQL.    \n",
    "- **Active community and documentation:** Benefit from community contributions, tutorials, and extensive documentation.    \n",
    "\n",
    "\n",
    "In this step we'll get all the image embeddings of *__kaggle__* dataset and store those embeddings into PostgreSQL vector type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84950ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "import boto3 \n",
    "import json \n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "response = client.get_secret_value(SecretId='apgpg-pgvector-secret')\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "dbconn = psycopg.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10, autocommit=True)\n",
    "\n",
    "dbconn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "register_vector(dbconn)\n",
    "\n",
    "dbconn.execute(\"DROP TABLE IF EXISTS similiarity_search;\")\n",
    "\n",
    "dbconn.execute(\"\"\"CREATE TABLE IF NOT EXISTS similiarity_search(\n",
    "                   id bigserial primary key, \n",
    "                   descriptions text, \n",
    "                   image_url text,\n",
    "                   display text,\n",
    "                   description_embeddings vector({}));\"\"\".format(no_of_embeddings))\n",
    "\n",
    "\n",
    "for _, x in df.iterrows():\n",
    "    dbconn.execute(\"\"\"INSERT INTO similiarity_search\n",
    "                  (descriptions, image_url, description_embeddings, display) \n",
    "                   VALUES(%s, %s, %s, %s);\"\"\", \n",
    "                   (x.get('descriptions'), x.get('image_url'), x.get('embeddings'),x.get('display')))\n",
    "\n",
    "dbconn.execute(\"\"\"CREATE INDEX ON similiarity_search \n",
    "                   USING hnsw (description_embeddings vector_cosine_ops) \n",
    "                   WITH  (m = 16, ef_construction = 64);\"\"\")\n",
    "\n",
    "dbconn.execute(\"VACUUM ANALYZE similiarity_search;\")\n",
    "\n",
    "dbconn.close()\n",
    "print (\"Vector embeddings has been successfully loaded into Aurora PostgreSQL tables \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a93851",
   "metadata": {},
   "source": [
    "## Evaluate PostgreSQL vector Search Results\n",
    "\n",
    "In this step we will use Amazon Bedrock to generate embeddings for the query and use the embeddings to search the PostgreSQL to retrive the nearest neighbours and retrive the relevent product images.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7e62d74",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def similarity_image_search(product):\n",
    "    \n",
    "    client = boto3.client('secretsmanager')\n",
    "    response = client.get_secret_value(SecretId='apgpg-pgvector-secret')\n",
    "    database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "    dbhost = database_secrets['host']\n",
    "    dbport = database_secrets['port']\n",
    "    dbuser = database_secrets['username']\n",
    "    dbpass = database_secrets['password']\n",
    "\n",
    "    dbconn = psycopg.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "    register_vector(dbconn)\n",
    "    \n",
    "    image_url = None\n",
    "\n",
    "    if type(product) is str:\n",
    "        r = dbconn.execute(\"\"\"SELECT image_url FROM similarity_search where lower(product_type) like '%'||lower('{}')||'%' limit 1;\"\"\".format(product))\n",
    "        result = r.fetchone()\n",
    "        if result is None:\n",
    "            print(\"No product type {} exists in the fashion table\".format(product))\n",
    "            return\n",
    "        image_url = result[0]\n",
    "    else:\n",
    "        image_url = df.iloc[product].get('ImageURL')\n",
    " \n",
    "    res1 = generate_embeddings(image_url)\n",
    "    \n",
    "    r = dbconn.execute(\"\"\"SELECT product_id, product_title, image_url, image_embeddings \n",
    "                            FROM fashion where image_url != %s\n",
    "                            ORDER BY image_embeddings <-> %s limit 3;\"\"\", (image_url, np.array(res1),))\n",
    "\n",
    " \n",
    "    urls = []\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    \n",
    "    display(Markdown(\"## Reference product\"))\n",
    "    display(HTML(\"\"\"<table><tr><td><img src={} width=\"250\"></td></tr></table>\"\"\".format(image_url)))\n",
    "    \n",
    "    display(Markdown((\"## Similar products\")))\n",
    "        \n",
    "    item_td = \"\"\n",
    "    img_td = \"\"\n",
    "    for x in r:\n",
    "        url = x[2]\n",
    "        item_td = item_td + \"\"\"<td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: {}</h4></td>\"\"\".format(str(x[0]))\n",
    "        img_td = img_td + \"\"\"<td><img src={} width=\"250\"></td>\"\"\".format(url)\n",
    "\n",
    "    display(HTML(\"\"\"<table><tr>{}</tr><tr>{}</tr></table>\"\"\".format(item_td,img_td)))\n",
    "    dbconn.close()\n",
    "    \n",
    "\n",
    "print(\"Search function created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "\n",
    "def similarity_search(search_text):\n",
    "    \n",
    "    embedding = numpy.array(generate_embeddings(search_text))\n",
    "    dbconn = psycopg.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "    register_vector(dbconn)\n",
    "    \n",
    "    r= dbconn.execute(\"\"\"SELECT id, image_url, display\n",
    "                         FROM similiarity_search \n",
    "                         ORDER BY description_embeddings <=> %s limit 3;\"\"\",(embedding,)).fetchall()\n",
    "   \n",
    "    img_td = \"\"\n",
    "    for x in r:\n",
    "        img_td = img_td + \"\"\"<tr><td><img src={} width=\"1000\"></td>\"\"\".format(x[1])\n",
    "        img_td = img_td + \"\"\"<td style=\"text-align: left; vertical-align: top;\"> <p>{}</p></td></tr>\"\"\".format(str(x[2]))\n",
    "       \n",
    "    display(HTML(\"\"\"<table>{}</table>\"\"\".format(img_td)))\n",
    "    dbconn.close()\n",
    "\n",
    "print(\"Created similarity_search function successfully\")\n",
    "\n",
    "similarity_search(\"Gift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19d572",
   "metadata": {},
   "source": [
    "Let's do some image search using the above function `similarity_image_search`. You can do image search by specifiying the item number or the product types.\n",
    "For example, you can pass 4 as item number or  \"Sport Shoes\" as product type as the parameter to the function.   \n",
    "You can try product types such as Tops, Dresses, Shorts, Tshirts, Jeans, Casual Shoes, Flip Flops, Formal Shoes , Sports Shoes etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search(\"red sleeveless summer wear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search(\"suggest something for december\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d700a",
   "metadata": {},
   "source": [
    "Let's search for 3rd image in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac744fa",
   "metadata": {},
   "source": [
    "Let's search for product type as \"Jeans\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ed8a8",
   "metadata": {},
   "source": [
    "In this workshop you have successfully implemented Image Search functionality in PostgreSQL using Amazon Bedrock and pgvector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
