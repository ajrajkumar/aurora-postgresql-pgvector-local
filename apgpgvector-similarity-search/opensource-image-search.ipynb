{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01cb58e",
   "metadata": {},
   "source": [
    "# Building AI-powered image search in PostgreSQL using Amazon Bedrock and pgvector\n",
    "_**Using a pretrained LLM and PostgreSQL extension `pgvector` for similarity image search on product catalog**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Amazon SageMaker Model Hosting](#Amazon-SageMaker-Model-Hosting)\n",
    "1. [Load data into PostgreSQL](#Open-source-extension-pgvector-in-PostgreSQL)\n",
    "1. [Evaluate Search Results](#Evaluate-PostgreSQL-vector-Search-Results)\n",
    "\n",
    "## Background\n",
    "\n",
    "Image search refers to the process of using an image as a query to find related or similar images. Image search is useful for various purposes, such as finding the source of an image, identifying objects or landmarks in a picture, or discovering visually similar content\n",
    "\n",
    "In this notebook, we'll build the core components of a visually similar Products. Often people don't know what exactly they are looking for and in that case they just type an item description or upload a photo of a product and looking for similar products matching those items.\n",
    "\n",
    "One of the core components of searching visually similar items is a fixed length sentence/word embedding i.e. a  “feature vector” that corresponds to that image. For image search, we will convert the product images into \"feature vector\" that corresponds to that image. The reference image embedding typically are generated offline and must be stored so they can be efficiently searched. In this use case we are using a pretrained Image model `tensorflow-icembedding-imagenet-inception-v2-featurevector-4` from [Tensorflow](https://tfhub.dev/google/imagenet/inception_v2/feature_vector/5). \n",
    "\n",
    "To enable efficient searches for visually similar items, we'll use Amazon SageMaker to generate fixed length sentence embeddings i.e “feature vectors” and use the Nearest Neighbor search in Amazon Aurora for PostgreSQL using the extension `pgvector`. The PostgreSQL `pgvector` extension lets you store and search for points in vector space and find the \"nearest neighbors\" for those points. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items:\n",
    "\n",
    "- Generate feature vectors for the products images from [Kaggle dataset]((https://www.kaggle.com/datasets/vikashrajluhaniwal/fashion-images/) using using Tensorflow Transformers.\n",
    "- Store the generated vectors in Amazon Aurora PostgreSQL with the pgvector extension along with the metadata\n",
    "- Explore some sample text queries, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7045906",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required python libraries for the workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb79cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgvector in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.34.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: scikit-image in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandarallel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.6.5)\n",
      "Requirement already satisfied: psycopg in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.1.17)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pgvector) (1.22.4)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.34.15)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (3.2)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (2.31.5)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (2023.9.26)\n",
      "Requirement already satisfied: packaging>=21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: dill>=0.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandarallel) (0.3.7)\n",
      "Requirement already satisfied: pandas>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandarallel) (2.1.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandarallel) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psycopg) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.15->boto3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1->pandarallel) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1->pandarallel) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.15->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pgvector tqdm boto3 requests scikit-image pillow pandarallel psycopg pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3cadd",
   "metadata": {},
   "source": [
    "### Downloading Fashion image dataset from Kaggle\n",
    "\n",
    "The dataset itself consists of 2900+  product images under Apparel and Footwear category. Two gender types Boys and Girls under Apparel, similarly Men and Women under Footwear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef202ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records : 2906\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Usage</th>\n",
       "      <th>ProductTitle</th>\n",
       "      <th>Image</th>\n",
       "      <th>ImageURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42419</td>\n",
       "      <td>Girls</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tops</td>\n",
       "      <td>White</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gini and Jony Girls Knit White Top</td>\n",
       "      <td>42419.jpg</td>\n",
       "      <td>http://assets.myntassets.com/v1/images/style/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34009</td>\n",
       "      <td>Girls</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Black</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gini and Jony Girls Black Top</td>\n",
       "      <td>34009.jpg</td>\n",
       "      <td>http://assets.myntassets.com/v1/images/style/p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductId Gender Category SubCategory ProductType Colour   Usage  \\\n",
       "0      42419  Girls  Apparel     Topwear        Tops  White  Casual   \n",
       "1      34009  Girls  Apparel     Topwear        Tops  Black  Casual   \n",
       "\n",
       "                         ProductTitle      Image  \\\n",
       "0  Gini and Jony Girls Knit White Top  42419.jpg   \n",
       "1       Gini and Jony Girls Black Top  34009.jpg   \n",
       "\n",
       "                                            ImageURL  \n",
       "0  http://assets.myntassets.com/v1/images/style/p...  \n",
       "1  http://assets.myntassets.com/v1/images/style/p...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data of csv\n",
    "df = pd.read_csv('data/fashion.csv')\n",
    "df = df[['ProductId','Gender','Category','SubCategory','ProductType','Colour','Usage','ProductTitle','Image','ImageURL']]\n",
    "print(\"Total number of records : {}\".format(len(df.index)))\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa2f4b",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Hosting\n",
    "\n",
    "In this section will deploy the pretrained `tensorflow-icembedding-imagenet-inception-v2-featurevector-4` model into SageMaker and generate 1024 dimensional vector embeddings for our product catalog images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c747e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::192355327736:role/genai-pgvector-lab-ExecutionRole-7bvQUcFhTZga\n",
      "sagemaker bucket: sagemaker-us-east-1-192355327736\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55f8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!Image Model has been deployed successfully to SageMaker\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "model_id = \"tensorflow-icembedding-imagenet-inception-v2-featurevector-4\"\n",
    "model_version = \"2.0.0\"\n",
    "endpoint_name = \"apg-image-vector\"\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base Tensorflow container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes the model and model parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "image_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "print(f\"Image Model has been deployed successfully to SageMaker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4718c2",
   "metadata": {},
   "source": [
    "Function to convert the image into vector embeddings. This function will be called for all the individual product images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3480d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image Dimensions : 1024\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_embeddings(url):\n",
    "    image_req = requests.get(url)\n",
    "    image_bytes = BytesIO(image_req.content)\n",
    "    image_embeddings_byte = image_model_predictor.predict(image_bytes,{\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },)\n",
    "    image_embeddings = json.loads(image_embeddings_byte.decode(\"utf-8\"))['embedding']\n",
    "    return image_embeddings\n",
    "\n",
    "image_embeddings = generate_embeddings(df.iloc[0].get('ImageURL'))\n",
    "\n",
    "print (\"Number of image Dimensions : {}\".format(len(image_embeddings)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfea36",
   "metadata": {},
   "source": [
    "In this code block, we will scan through all the data in the dataframe for the image stored in the ImageURL column and convert it as embeddings using TensorFlow Transformer and store it as image_embeddings column in the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7073d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e1b4d66aea4352be03c70f5f9b808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=364), Label(value='0 / 364'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generation of embeddings for all the products images\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all the products descriptions - approx 3 min to complete\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
    "\n",
    "df['image_embeddings'] = df['ImageURL'].parallel_apply(generate_embeddings)\n",
    "df.head()\n",
    "\n",
    "print(\"Completed generation of embeddings for all the products images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c5735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\n",
      "Collecting torch>=1.6.0 (from sentence_transformers)\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision (from sentence_transformers)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m969.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=1b6e7d17f474898cb861d210517ea8e5ebcaac040b67da6d9717f251350f4b67\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchvision, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.20.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.15.0 torch-2.1.2 torchvision-0.16.2 transformers-4.36.2 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55bd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "\n",
    "#First, we load the respective CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ad400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "response = requests.get(df.iloc[0].get('ImageURL'))\n",
    "query = Image.open(BytesIO(response.content))\n",
    "\n",
    "#query = Image.open(os.path.join(img_folder, 'lyStEjlKNSw.jpg'))\n",
    "\n",
    "query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False).tolist()[0]\n",
    "print(len(query_emb[0]))\n",
    "#print(query_emb)\n",
    "#print(type(query_emb))\n",
    "#print(query_emb.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eabaa005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image Dimensions : 512\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_embeddings(url):\n",
    "    image_req = requests.get(url)\n",
    "    image_bytes = Image.open(BytesIO(image_req.content))\n",
    "    image_embeddings = model.encode([image_bytes], convert_to_tensor=True, show_progress_bar=False).tolist()[0]\n",
    "    return image_embeddings\n",
    "\n",
    "image_embeddings = generate_embeddings(df.iloc[0].get('ImageURL'))\n",
    "\n",
    "print (\"Number of image Dimensions : {}\".format(len(image_embeddings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c68552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e59095facf34554865f80229941fe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generation of embeddings for all the products images\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all the products descriptions - approx 3 min to complete\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "#pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
    "\n",
    "#df['image_embeddings'] = df['ImageURL'].parallel_apply(generate_embeddings)\n",
    "df['image_embeddings'] = df['ImageURL'].progress_apply(generate_embeddings)\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"Completed generation of embeddings for all the products images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e107d",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector in PostgreSQL\n",
    "\n",
    "pgvector is an open-source extension for PostgreSQL that allows you to store and search vector embeddings for exact and approximate nearest neighbors. It is designed to work seamlessly with other PostgreSQL features, including indexing and querying.\n",
    "\n",
    "One of the key benefits of using pgvector is that it allows you to perform similarity searches on large datasets quickly and efficiently. This is particularly useful in industries like e-commerce, where businesses need to be able to quickly search through large product catalogs to find the items that best match a customer's preferences. It supports exact and approximate nearest neighbor search, L2 distance, inner product, and cosine distance.\n",
    "\n",
    "To further optimize your searches, you can also use pgvector's indexing features. By creating indexes on your vector data, you can speed up your searches and reduce the amount of time it takes to find the nearest neighbors to a given vector.\n",
    "\n",
    "In this step we'll get all the image embeddings of *__kaggle__* dataset and store those embeddings into PostgreSQL vector type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84950ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector embeddings has been successfully loaded into Aurora PostgreSQL tables \n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "import boto3 \n",
    "import json \n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "response = client.get_secret_value(SecretId='apgpg-pgvector-secret')\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "dbconn = psycopg.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10, autocommit=True)\n",
    "\n",
    "dbconn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "register_vector(dbconn)\n",
    "\n",
    "dbconn.execute(\"DROP TABLE IF EXISTS fashion;\")\n",
    "\n",
    "dbconn.execute(\"\"\"CREATE TABLE IF NOT EXISTS fashion(\n",
    "                   id bigserial primary key, \n",
    "                   product_id text, \n",
    "                   category text, \n",
    "                   product_type text, \n",
    "                   product_title text,\n",
    "                   image_url text,\n",
    "                   image_embeddings vector(512));\"\"\")\n",
    "\n",
    "\n",
    "for _, x in df.iterrows():\n",
    "    dbconn.execute(\"\"\"INSERT INTO fashion\n",
    "                  (product_id, category, product_type, product_title, image_url, image_embeddings) \n",
    "                   VALUES(%s, %s, %s, %s, %s, %s);\"\"\", \n",
    "                   (x.get('ProductId'), x.get('Category'), x.get('ProductType'), x.get('ProductTitle'), x.get('ImageURL'), x.get('image_embeddings')))\n",
    "\n",
    "dbconn.execute(\"\"\"CREATE INDEX ON fashion \n",
    "                   USING hnsw (image_embeddings vector_cosine_ops) \n",
    "                   WITH  (m = 16, ef_construction = 64);\"\"\")\n",
    "\n",
    "dbconn.execute(\"VACUUM ANALYZE fashion;\")\n",
    "\n",
    "dbconn.close()\n",
    "print (\"Vector embeddings has been successfully loaded into Aurora PostgreSQL tables \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a93851",
   "metadata": {},
   "source": [
    "## Evaluate PostgreSQL vector Search Results\n",
    "\n",
    "In this step we will use SageMaker realtime inference to generate embeddings for the query and use the embeddings to search the PostgreSQL to retrive the nearest neighbours and retrive the relevent product images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7606320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search function created successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def similarity_image_search(image_url):\n",
    "    res1 = generate_embeddings(image_url)\n",
    "    \n",
    "    client = boto3.client('secretsmanager')\n",
    "    response = client.get_secret_value(SecretId='apgpg-pgvector-secret')\n",
    "    database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "    dbhost = database_secrets['host']\n",
    "    dbport = database_secrets['port']\n",
    "    dbuser = database_secrets['username']\n",
    "    dbpass = database_secrets['password']\n",
    "\n",
    "    dbconn = psycopg.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "    register_vector(dbconn)\n",
    "        \n",
    "    r = dbconn.execute(\"\"\"SELECT product_id, product_title, image_url, image_embeddings \n",
    "                            FROM fashion where image_url != %s\n",
    "                            ORDER BY image_embeddings <-> %s limit 3;\"\"\", (image_url, np.array(res1),))\n",
    " \n",
    "    urls = []\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    \n",
    "    display(Markdown(\"## Reference product\"))\n",
    "    display(HTML(\"\"\"<table><tr><td><img src={} width=\"250\"></td></tr></table>\"\"\".format(image_url)))\n",
    "    \n",
    "    display(Markdown((\"## Similar products\")))\n",
    "        \n",
    "    item_td = \"\"\n",
    "    img_td = \"\"\n",
    "    for x in r:\n",
    "        url = x[2]\n",
    "        item_td = item_td + \"\"\"<td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: {}</h4></td>\"\"\".format(str(x[0]))\n",
    "        img_td = img_td + \"\"\"<td><img src={} width=\"250\"></td>\"\"\".format(url)\n",
    "\n",
    "    display(HTML(\"\"\"<table><tr>{}</tr><tr>{}</tr></table>\"\"\".format(item_td,img_td)))\n",
    "    dbconn.close()\n",
    "    \n",
    "\n",
    "print(\"Search function created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19d572",
   "metadata": {},
   "source": [
    "Using the  function `similarity_image_search` , lets do some image search. \n",
    "For example, the parameter passed to the function is the reference image URL for the 4th item in the list out of 8000+ items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7bf574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Reference product"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/ef9685293a987f515492addd034006bf_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Similar products"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 52123</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 50721</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 38286</h4></td></tr><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/79e71e6bb1a3969bbb6994aef1e6d87e_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/8cb3dbe21deed6e53c4eaca084d65c00_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/c66234621743217b035b79ace645afd1_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_image_search(df.iloc[3].get('ImageURL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac744fa",
   "metadata": {},
   "source": [
    "Let's search for Jeans similar to the image in the 1004th row of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80ca1521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Reference product"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/8d502c198e871b2dd94f96d0c27c0430_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Similar products"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 38993</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 40928</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 40925</h4></td></tr><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/829c49c4d8b2a8827ad0d97efc685dc9_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/df57dc0bbff5f614f9f8f104fd4c0f02_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/6303d1b19005036148228a9f8ae5bbab_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_image_search(df.iloc[1003].get('ImageURL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41cdc8",
   "metadata": {},
   "source": [
    "Let's search for Shoes similar to the image in the 2001th row of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e39649cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Reference product"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/f19436ed7b41f8ee107c2ff56e1cdf13_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Similar products"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 3301</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 33822</h4></td><td style=\"text-align: center; vertical-align: middle;\"> <h4> ProductId: 13213</h4></td></tr><tr><td><img src=http://assets.myntassets.com/v1/images/style/properties/Asics-Women-Gel-Cushioning-White-Pink-Shoe_6d9b860f1d884359880b5180dc1e1544_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/52ad8ae288ba080d88a116b3c70305ec_images.jpg width=\"250\"></td><td><img src=http://assets.myntassets.com/v1/images/style/properties/ddc590c26bb73da101036df26d75cc88_images.jpg width=\"250\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_image_search(df.iloc[2000].get('ImageURL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ed8a8",
   "metadata": {},
   "source": [
    "In this workshop you have successfully implemented Image Search functionality in PostgreSQL using Amazon Bedrock and pgvector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
